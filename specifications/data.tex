\subsubsection{Precision, Granularity, and Availability of Data}
In considering the question of acceptable latency we were also forced to confront tradeoffs relating to network traffic. At first glance it would seem that in the ideal case perfectly precise information transmitted by each \netlet would become available to its user via our website instantaneously. Setting aside the fact that all networked communications are subject to a greater or lesser degree of latency in transmission, perfectly precise information transmitted at a rate sufficiently high as to approximate the continuous variation of current would consume enough bandwidth (in the sense of bits of information transmitted per unit time) it would overwhelm both the network and our server. Furthermore, sending information at a rate greater than the \si{120}{\hertz} Nyquist frequency associated with the \si{50-60}{\hertz} frequency of the alternating current in mains power would make RMS computation of power consumption meaningless. On top of all this, network connectivity often drops out and resumes. Our design decision became how precisely and frequently to measure current on the side of the \netlet, how much data to cache in the event of a communications dropout, and how frequently to attempt to send data to the server.\\

In order to compute RMS current the \atmega needs to sample at a rate greater than twice the Nyquist frequency associated with a \si{50-60}{\hertz} signal---\si{120}{\hertz}. Because the \atmega is a fairly capable microcontroller we were able to do considerably better than this, sampling at \si{1}{\kilohertz}. By monitoring zero crossings the microcontroller is able to compensate for variations in both sampling frequency and signal frequency (within reasonable margins). But, as outlined above, it is neither necessary nor desirable to send information from the \atmega boards to the main \arm board even at every zero crossing. Instead, the \atmega maintains a running average until the \arm board requests a sample, which occurs every \si{30}{\second}. Every time the \arm board requests a sample it attempts to send the timestamped sample to the server. If this is not possible (i.e. in the event of a network dropout) the \netlet caches the sample and tries to send it later, discarding old samples if the cache grows too large.\\

From the user's perspective these samples become available shortly after the server receives them, represented as a curve on a constantly updating chart whose timescale is easily adjustable with an intuitive widget. Thus, during ideal network conditions, users see changes to power consumption every \si{30}{\second}. In our tests this interval proved adequate from a user experience perspective and did not overwhelm either the network or the server.\\